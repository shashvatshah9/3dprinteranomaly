{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-17 01:14:22.382442: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms, models\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification, Trainer, TrainingArguments\n",
    "import datasets\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "root_path = '/scratch/sss9772/early-detection-of-3d-printing-issues/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset my_dataset (/scratch/sss9772/.cache/my_dataset/default/0.0.0/71801c53399c6f25b1f58897c3a6286da73c00f60a6b6b86daa89653e3b29e8e)\n",
      "Loading cached shuffled indices for dataset at /scratch/sss9772/.cache/my_dataset/default/0.0.0/71801c53399c6f25b1f58897c3a6286da73c00f60a6b6b86daa89653e3b29e8e/cache-c8317d096051a40a.arrow\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/vit-base-patch16-224\"\n",
    "\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "        transforms.RandomRotation(60),\n",
    "        transforms.ColorJitter(brightness=(0.5,1.5),contrast=(1),saturation=(0.5,1.5),hue=(-0.1,0.1)),\n",
    "        transforms.Resize((400,400), interpolation=transforms.InterpolationMode.NEAREST)\n",
    "])\n",
    "\n",
    "def vit_transform(example_batch):\n",
    "    example_batch['image'] = [img_transform(img) for img in example_batch['image']]\n",
    "    inputs = feature_extractor([x for x in example_batch['image']], return_tensors='pt')\n",
    "    inputs['label'] = example_batch['label']\n",
    "    return inputs\n",
    "\n",
    "dataset = datasets.load_dataset(os.path.join(root_path, 'my_dataset.py'), num_proc=8, split='train', cache_dir=\"/scratch/sss9772/.cache\")\n",
    "dataset = dataset.cast_column(\"image\", datasets.Image())\n",
    "dataset = dataset.shuffle(seed=SEED)\n",
    "dataset.set_format('torch')\n",
    "dataset.set_transform(vit_transform)\n",
    "\n",
    "train_test = dataset.train_test_split(0.1, stratify_by_column='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ViTForImageClassification.from_pretrained(model_name, num_labels=2, ignore_mismatched_sizes=True)\n",
    "model.classifier = torch.nn.Linear(in_features=768, out_features=2, bias=True)\n",
    "model.classifier.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "number = 5\n",
    "run_name = 'run_' + str(number)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './output_' + str(number),\n",
    "    num_train_epochs=4,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    per_device_eval_batch_size= 64,\n",
    "    gradient_accumulation_steps = 4,    \n",
    "    evaluation_strategy = 'steps',\n",
    "    save_strategy='steps',\n",
    "    eval_steps=50,\n",
    "    save_steps=50,\n",
    "    disable_tqdm = False, \n",
    "    warmup_steps=50,\n",
    "    logging_steps = 50,\n",
    "    logging_dir='./logs_' + str(number),\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers = 8,\n",
    "    run_name = run_name,\n",
    "    report_to='wandb',\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2,\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(predictor):\n",
    "    labels = predictor.label_ids\n",
    "    preds = predictor.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    return {\n",
    "        'precision' : precision,\n",
    "        'recall' : recall,\n",
    "        'f1' : f1,\n",
    "        'accuracy': acc\n",
    "    }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['label'] for x in batch])\n",
    "    }\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        inp = inputs.get('pixel_values')\n",
    "        outputs = model(inp)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss = torch.nn.CrossEntropyLoss()(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss    \n",
    "\n",
    "trainer = CustomTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            data_collator=collate_fn,\n",
    "            compute_metrics=compute_metrics,\n",
    "            train_dataset=train_test['train'],\n",
    "            eval_dataset=train_test['test'],\n",
    "            tokenizer=feature_extractor\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='207' max='504' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [207/504 28:35 < 41:24, 0.12 it/s, Epoch 1.63/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.063900</td>\n",
       "      <td>0.069612</td>\n",
       "      <td>0.866738</td>\n",
       "      <td>0.681345</td>\n",
       "      <td>0.740359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.060400</td>\n",
       "      <td>0.055867</td>\n",
       "      <td>0.865492</td>\n",
       "      <td>0.770092</td>\n",
       "      <td>0.810038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.053400</td>\n",
       "      <td>0.053156</td>\n",
       "      <td>0.886440</td>\n",
       "      <td>0.790355</td>\n",
       "      <td>0.831027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.046700</td>\n",
       "      <td>0.043909</td>\n",
       "      <td>0.870889</td>\n",
       "      <td>0.845732</td>\n",
       "      <td>0.857830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'vit_run_1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.state.best_model_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = ViTForImageClassification.from_pretrained(trainer.state.best_model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def vit_transform_test(example_batch):\n",
    "    inputs = feature_extractor([x for x in example_batch['image']], return_tensors='pt')\n",
    "    inputs['image_path'] = example_batch['image_path']\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = datasets.load_dataset(os.path.join(root_path, 'my_dataset_test.py'), num_proc=8, split='test', cache_dir=\"/scratch/sss9772/.cache\")\n",
    "test_dataset = test_dataset.cast_column(\"image\", datasets.Image())\n",
    "test_dataset.set_format('torch')\n",
    "test_dataset.set_transform(vit_transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16, num_workers = 8, shuffle=False)\n",
    "finetuned_model.to(device)\n",
    "finetuned_model.eval()\n",
    "res = []\n",
    "for i,data in enumerate(tqdm(test_dataloader)):\n",
    "    pixel_values = data['pixel_values'].to(device)\n",
    "    preds_probs = finetuned_model(pixel_values)\n",
    "    preds_class = torch.argmax(preds_probs['logits'], dim=-1)\n",
    "    preds_class = preds_class.cpu().detach().numpy()\n",
    "    for i in range(len(data['image_path'])):\n",
    "        res.append([data['image_path'][i], int(preds_class[i])])\n",
    "\n",
    "sub_file = 'submission'+ str(time.time()) +'.csv'\n",
    "df = pd.DataFrame(res, columns = ['img_path', 'has_under_extrusion'])\n",
    "df.to_csv(sub_file, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "my_env_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
